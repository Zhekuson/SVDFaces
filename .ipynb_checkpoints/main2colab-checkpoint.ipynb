{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "dpHeiKSv5DhF"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sympy as sp\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HWRkf8-_6K4F",
    "outputId": "f7743e1e-2cc6-4d34-8596-7b36691e705c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "eBi4yIrb6V6j"
   },
   "outputs": [],
   "source": [
    "!rm 'vggdataset4/' -r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqVKbFpP6TFo"
   },
   "outputs": [],
   "source": [
    "!unzip 'drive/MyDrive/SVDproj/vggimageslittle.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SX4nAJxM5DhG"
   },
   "outputs": [],
   "source": [
    "def get_sigma_matrix(M):\n",
    "    '''\n",
    "    returns sigma and \n",
    "    1) in case MMT -> U\n",
    "    2) in case MTM -> V\n",
    "    '''\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(M)\n",
    "    sort_eigenvalues = np.sort(eigenvalues)[::-1]\n",
    "    indexes = np.argsort(eigenvalues)[::-1]\n",
    "    Sigma = np.sqrt(sort_eigenvalues)\n",
    "    matrix = eigenvectors[:,indexes]\n",
    "    return Sigma, matrix\n",
    "def custom_SVD(M):\n",
    "    '''\n",
    "    returns U, Sigma(vector), V^T\n",
    "    '''\n",
    "    MTM = np.transpose(M) @ M\n",
    "    MMT = M @ np.transpose(M)\n",
    "    # m x n  \n",
    "    if (M.shape[0] < M.shape[1]):\n",
    "        # m < n\n",
    "        Sigma, U = get_sigma_matrix(MMT)\n",
    "        #search V\n",
    "        _, V = get_sigma_matrix(MTM)\n",
    "    else:\n",
    "        # n <= m\n",
    "        Sigma, V = get_sigma_matrix(MTM)\n",
    "        # search U \n",
    "        _, U = get_sigma_matrix(MMT) \n",
    "    return U, Sigma, V.transpose()\n",
    "def get_full_sigma(Sigma, A):\n",
    "    result = np.zeros(A.shape)\n",
    "    for i in range(np.min(A.shape)):\n",
    "        result[i][i] = Sigma[i]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l3vuUOKr5DhG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NjzsqJw35DhG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3-vO9wk05DhG",
    "outputId": "1588ec77-2429-4c80-ab77-5f10e656f683"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0],\n",
       "       [0, 0, 0, 4],\n",
       "       [0, 3, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [2, 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.conjugate().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AG5dB30U5DhI",
    "outputId": "dd4355b0-70d4-4c1f-c8da-7c97f2b46e2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 1., 1.],\n",
       "       [1., 3., 1.],\n",
       "       [1., 1., 5.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.diag([1,1,3])\n",
    "A = A + np.eye(3) + np.ones_like(A)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sM2i-L0B5DhJ",
    "outputId": "c2c6ac8f-5bb2-402f-ade8-e5e3f1722615"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-4.08248290e-01,  5.77350269e-01,  7.07106781e-01],\n",
       "        [-4.08248290e-01,  5.77350269e-01, -7.07106781e-01],\n",
       "        [-8.16496581e-01, -5.77350269e-01,  2.68773248e-16]]),\n",
       " array([6., 3., 2.]),\n",
       " array([[-4.08248290e-01, -4.08248290e-01, -8.16496581e-01],\n",
       "        [ 5.77350269e-01,  5.77350269e-01, -5.77350269e-01],\n",
       "        [ 7.07106781e-01, -7.07106781e-01,  1.65064933e-16]]))"
      ]
     },
     "execution_count": 52,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.svd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdiXSnGa5DhJ",
    "outputId": "e17aa73e-19c8-4e69-96f7-b6ae52acdcf6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-4.08248290e-01, -5.77350269e-01, -7.07106781e-01],\n",
       "        [-4.08248290e-01, -5.77350269e-01,  7.07106781e-01],\n",
       "        [-8.16496581e-01,  5.77350269e-01, -1.59560649e-16]]),\n",
       " array([6., 3., 2.]),\n",
       " array([[-4.08248290e-01, -4.08248290e-01, -8.16496581e-01],\n",
       "        [-5.77350269e-01, -5.77350269e-01,  5.77350269e-01],\n",
       "        [-7.07106781e-01,  7.07106781e-01, -1.59560649e-16]]))"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_SVD(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pn8BSXe15DhJ"
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, 0, 0, 0, 2],\n",
    "      [0, 0, 3, 0, 0],\n",
    "      [0, 0, 0, 0, 0],\n",
    "      [0, 4, 0, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eb7pzNgK5DhJ",
    "outputId": "ba4ea9c4-e96a-43c6-ef36-f98a44240004"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., -1.],\n",
       "        [ 1.,  0.,  0.,  0.]]),\n",
       " array([4.        , 3.        , 2.23606798, 0.        ]),\n",
       " array([[-0.        ,  1.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [-0.        ,  0.        ,  1.        ,  0.        ,  0.        ],\n",
       "        [ 0.4472136 ,  0.        ,  0.        ,  0.        ,  0.89442719],\n",
       "        [ 0.        ,  0.        ,  0.        ,  1.        ,  0.        ],\n",
       "        [-0.89442719,  0.        ,  0.        ,  0.        ,  0.4472136 ]]))"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.svd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zD2wFkEH5DhJ",
    "outputId": "a3a71c5a-f41e-4c6e-c353-bb548bd80ed6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.,  0.,  0.,  0., -2.],\n",
       "       [ 0.,  0.,  3.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  4.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U, S, V = custom_SVD(A)\n",
    "\n",
    "U @ get_full_sigma(S,A) @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8IqWGJf5DhK",
    "outputId": "de0ade30-df4c-47ab-b9dc-9fc2217e6af9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 3.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 2.23606798, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_full_sigma(S, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PzXJx4mJ5DhK",
    "outputId": "603a642a-8c9f-4529-a34f-36cd49e25829"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]] [4.         3.         2.23606798 0.        ] [[ 0.          1.          0.          0.          0.        ]\n",
      " [ 0.          0.          1.          0.          0.        ]\n",
      " [-0.4472136   0.          0.          0.         -0.89442719]\n",
      " [ 0.          0.          0.          1.          0.        ]\n",
      " [-0.89442719  0.          0.          0.          0.4472136 ]]\n"
     ]
    }
   ],
   "source": [
    "print(U, S, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "DQouulQK5DhK"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "import torchvision.transforms as transforms\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "qDMJI0eR5DhK"
   },
   "outputs": [],
   "source": [
    "class SVDtransform(object):\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample[0], sample[1]\n",
    "        U, sigma, V = np.linalg.svd(image)\n",
    "        return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "lXlyu4jI5DhK"
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "RMXT5rD45DhK",
    "outputId": "c61fc4f9-ea5d-46d7-bb88-3dd797c7ea27"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-f90a142a2204>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPILToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVDtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPILToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSVDtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./train/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./test/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    227\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    106\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m    107\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mNo\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msubdirectory\u001b[0m \u001b[0mof\u001b[0m \u001b[0manother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mcls_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './train/'"
     ]
    }
   ],
   "source": [
    "train_transform = transforms.Compose([transforms.Resize(512), transforms.PILToTensor(), SVDtransform()])\n",
    "test_transform = transforms.Compose([transforms.Resize(512), transforms.PILToTensor(), SVDtransform()])\n",
    "train_dataset = torchvision.datasets.ImageFolder('./train/', transform=train_transform)\n",
    "test_dataset = torchvision.datasets.ImageFolder('./test/', transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ita9t5c5DhK",
    "outputId": "58dc6f13-5160-46c0-806c-d9c5ceaca3c7"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-e4a7cc5678f6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mknn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[0mtuple\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mclass_index\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;32mclass\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \"\"\"\n\u001b[1;32m--> 136\u001b[1;33m         \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m         \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "knn.fit(train_dataset[:][0],train_dataset[:][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "vEBfEw3v5DhL"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "class CustomDataset():\n",
    "    def __init__(self, root, W, H):\n",
    "        self.W = W\n",
    "        self.H = H\n",
    "        self.paths = []\n",
    "        self.labels = []\n",
    "        counter = 0\n",
    "        for directory in os.listdir(root):\n",
    "          for file in os.listdir(root+'/'+directory):\n",
    "            self.paths.append(root+'/'+ directory + '/' + file)\n",
    "            self.labels.append(counter)\n",
    "          counter += 1\n",
    "    def __getitem__(self, index):\n",
    "        img = PIL.Image.open(self.paths[index])\n",
    "        img = img.resize((self.W, self.H))\n",
    "        _, sigma, v = np.linalg.svd(img)\n",
    "        return np.array([sigma, self.labels[index]])\n",
    "    def get_images(self):\n",
    "      result = []\n",
    "      for path in tqdm(self.paths):\n",
    "        img = PIL.Image.open(path)\n",
    "        img = img.resize((self.W, self.H))\n",
    "        _, sigma, v = np.linalg.svd(img)\n",
    "        result.append(sigma)\n",
    "      return np.array(result)\n",
    "    def get_labels(self):\n",
    "      return np.array(self.labels)\n",
    "    def __len__(self):\n",
    "      return len(self.paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "A4ppGvzCjjbP"
   },
   "outputs": [],
   "source": [
    "class CustomDatasetWithLoading():\n",
    "    def __init__(self, root, W, H):\n",
    "        self.root = root\n",
    "        self.W = W\n",
    "        self.H = H\n",
    "        counter = 0\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        for directory in tqdm(os.listdir(root)):\n",
    "            for file in tqdm(os.listdir(root+'/'+directory)):\n",
    "                img = PIL.Image.open(root+'/'+ directory + '/' + file)\n",
    "                img = img.resize((self.W, self.H))\n",
    "                _, sigma, v = np.linalg.svd(img)\n",
    "                self.images.append(sigma)\n",
    "                self.labels.append(counter)\n",
    "            counter += 1\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return np.array([self.images[index], self.labels[index]])\n",
    "    def get_images(self):\n",
    "      return np.array(self.images)\n",
    "    def get_labels(self):\n",
    "      return np.array(self.labels)\n",
    "    def __len__(self):\n",
    "      return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCqYrKdv5DhL"
   },
   "outputs": [],
   "source": [
    "#train_dataset = CustomDataset('vggimageslittle/train/', 512, 512)\n",
    "train_dataset = CustomDatasetWithLoading('vggimageslittle/train/', 512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "H7h1rfs35DhL"
   },
   "outputs": [],
   "source": [
    "#test_dataset = CustomDataset('vggimageslittle/test/', 512, 512)\n",
    "test_dataset = CustomDatasetWithLoading('vggimageslittle/test/', 512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UsyE0tpi5DhL",
    "outputId": "f8536b7b-de1a-4468-be37-0078c535edc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23253"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8s_VwfBk5DhL",
    "outputId": "605d7bda-2c5f-4339-b15c-90c2f0ecaeaa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([[1683.27983826,  226.75314931,   66.00754035],\n",
       "       [1668.80270461,  226.03630842,   66.1446926 ],\n",
       "       [1638.48856799,  225.18668639,   65.75080875],\n",
       "       ...,\n",
       "       [8549.94055907,  230.82530229,   78.25034263],\n",
       "       [8572.48154475,  232.59054553,   79.25151817],\n",
       "       [8582.62593064,  234.40415919,   79.9926547 ]]),\n",
       "       0], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "nCR518HE5DhL"
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "K5b44Biy5DhL"
   },
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dIbWnglPjblt"
   },
   "outputs": [],
   "source": [
    "model.fit(train_dataset.get_images(), train_dataset.get_labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qU9DBR_55DhL"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1ngMb_FUXxx"
   },
   "outputs": [],
   "source": [
    "accuracy_score(test_dataset.get_labels(),model.predict(test_dataset.get_images()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uj2_6Ad1Kl8I"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle\n",
    "pkl_filename = \"drive/MyDrive/SVDproj/linearmodel.pkl\"\n",
    "with open(pkl_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RQ8972BNKl_Z"
   },
   "outputs": [],
   "source": [
    "with open(pkl_filename, 'rb') as file:\n",
    "    pickle_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lAvu3buoKmBy",
    "outputId": "6e567274-c748-4a11-d657-2030bd507254"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "jQBZXNAGg4km"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import torch.optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "L5-_RBCOg4oo"
   },
   "outputs": [],
   "source": [
    "faces_count = 4\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, faces_count):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, faces_count)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_jJOjMZjHAS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "8uTYDPiMimWe"
   },
   "outputs": [],
   "source": [
    "train_transform = T.Compose([T.Resize(512), T.ToTensor(), SVDtransform()])\n",
    "test_transform = T.Compose([T.Resize(512), T.ToTensor(), SVDtransform()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "agJmlh15g4rX"
   },
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder('vggimageslittle/train/', transform=train_transform)\n",
    "test_dataset = torchvision.datasets.ImageFolder('vggimageslittle/test/', test_transform) \n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=200, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=200, shuffle=True)\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "BAotEys_g4tv"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_dataloader, criterion, optimizer, device):\n",
    "  model = model.to(device)\n",
    "  for images, labels in train_dataloader:\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "      y_pred = model.forward(images)\n",
    "      loss = criterion(y_pred, labels)\n",
    "      print(loss)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "def predict(model, val_dataloader, device):\n",
    "    model = model.to(device)\n",
    "    n = 0\n",
    "    right = 0\n",
    "    for images, labels in val_dataloader:\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "      y_pred = model.forward(images)\n",
    "      for i in range(len(images)):\n",
    "        if (torch.argmax(y_pred[i]) == labels[i]):\n",
    "          right += 1\n",
    "      n += len(images)  \n",
    "    return right/n\n",
    "def train(model, train_dataloader, val_dataloader, criterion, optimizer, device, n_epochs=10):\n",
    "  for n in tqdm(range(n_epochs)):\n",
    "    train_one_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    print(predict(model,val_dataloader,device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyJLKJjGg4v-"
   },
   "outputs": [],
   "source": [
    "model = LinearModel(faces_count)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train(model, train_dataloader, test_dataloader, criterion, optimizer, device, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tUgRZ7tkyZZC",
    "outputId": "bb61be78-239b-4592-d7eb-1a2aed49b771"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 25%|██▌       | 1/4 [00:29<01:27, 29.26s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 50%|█████     | 2/4 [00:58<00:58, 29.29s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      " 75%|███████▌  | 3/4 [01:27<00:29, 29.30s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 4/4 [01:41<00:00, 25.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014492753623188406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(predict(model,test_dataloader,device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pPgzF42TlV7A"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'/drive/MyDrive/SVDproj/linearsvd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "id": "z5VTbumB4Dsh"
   },
   "outputs": [],
   "source": [
    "!rm ORL -r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0uMA0Xio6zs"
   },
   "source": [
    "#### VGG4 distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5N-xBQO4Dvm"
   },
   "outputs": [],
   "source": [
    "!unzip 'drive/MyDrive/SVDproj/vggdataset4.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "Jp-q5xRR4Dye"
   },
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.ImageFolder('vggdataset4/train/', transform=train_transform)\n",
    "test_dataset = torchvision.datasets.ImageFolder('vggdataset4/test/', test_transform) \n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=1, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=1, shuffle=True)\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yrlujs8h4Wbz"
   },
   "outputs": [],
   "source": [
    "model = LinearModel(4)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train(model, train_dataloader, test_dataloader, criterion, optimizer, device, n_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "id": "KQmCQc8Mbaqa"
   },
   "outputs": [],
   "source": [
    "class SVDtransform(object):\n",
    "    def __call__(self, sample):\n",
    "        print(sample)\n",
    "        image, label = sample[0], sample[1]\n",
    "        U, sigma, V = np.linalg.svd(image)\n",
    "        return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dl-xkaX4Wfa"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "faces_count = 4\n",
    "Size = 200\n",
    "train_transform = T.Compose([T.Grayscale(), T.CenterCrop((Size, Size)),\n",
    "                                         T.ToTensor(),\n",
    "                             T.Normalize((0.5), (0.5))])\n",
    "test_transform = T.Compose([T.Grayscale(), T.CenterCrop((Size, Size)), \n",
    "                            T.ToTensor(),\n",
    "                            T.Normalize((0.5), (0.5))])\n",
    "train_dataset = torchvision.datasets.ImageFolder('vggdataset4/train/', transform=train_transform)\n",
    "test_dataset = torchvision.datasets.ImageFolder('vggdataset4/test/', test_transform) \n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=1, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=1, shuffle=True)\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "cluster_vec = np.array(np.zeros([4,Size]))\n",
    "sumvec = np.array(np.empty([4,Size], dtype=float))\n",
    "counters = np.array(np.zeros(shape=(4)))\n",
    "for image, label in tqdm(train_dataloader):\n",
    "    image = image.reshape((Size,Size)).numpy()\n",
    "    U, sigma, V = np.linalg.svd(image)\n",
    "    image = sigma\n",
    "    sigma = sigma/np.linalg.norm(sigma) \n",
    "    sumvec[label] += image\n",
    "    counters[label] += 1\n",
    "for i in range(faces_count):\n",
    "  cluster_vec[i] = sumvec[i]/ np.linalg.norm(sumvec[i]) #counters[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwUgCGH04WiG"
   },
   "outputs": [],
   "source": [
    "cluster_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xfpBIE2JHNpP",
    "outputId": "d663d9e2-d736-4159-9e30-3676eb2511f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30., 30., 30., 30.])"
      ]
     },
     "execution_count": 261,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "id": "RkzKw-0w4WkY"
   },
   "outputs": [],
   "source": [
    "def distance(vec1, vec2):\n",
    "  return np.sqrt(np.sum((vec1-vec2)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "id": "j1hm0fhbGb9M"
   },
   "outputs": [],
   "source": [
    "def distanceM(vec1, vec2):\n",
    "  return np.sum(np.abs(vec1-vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qINtkEWzElGG"
   },
   "outputs": [],
   "source": [
    "count = len(test_dataloader)\n",
    "right = 0\n",
    "for image, label in test_dataloader:\n",
    "  image = image.reshape((Size, Size)).numpy()\n",
    "  distances = []\n",
    "  for i in range(faces_count):\n",
    "    distances.append(distance(cluster_vec[i], image))\n",
    "  distances = np.array(distances)\n",
    "  if (np.argmin(distances) == label):\n",
    "    right += 1\n",
    "  print(distances)\n",
    "  print(np.argmin(distances))\n",
    "print(right/count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {
    "id": "sL2FgaszJOU_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import torch.optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBo4ofKYJOXp"
   },
   "outputs": [],
   "source": [
    "!unzip 'drive/MyDrive/SVDproj/ORL.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "id": "WYcf3VitJOaa"
   },
   "outputs": [],
   "source": [
    "faces_count = 7\n",
    "W = 112\n",
    "H = 92\n",
    "top_values = 10\n",
    "train_transform = T.Compose([T.Grayscale(),T.ToTensor(), T.Normalize((0.5), (0.5))])\n",
    "test_transform = T.Compose([T.Grayscale(),T.ToTensor(), T.Normalize((0.5), (0.5))])\n",
    "train_dataset = torchvision.datasets.ImageFolder('ORL/train/', transform = train_transform)\n",
    "test_dataset = torchvision.datasets.ImageFolder('ORL/test/', transform = test_transform) \n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=1, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=1, shuffle=True)\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "average_matrices = np.array(np.zeros([faces_count,W,H]))\n",
    "counters = np.array(np.zeros(shape=(faces_count)))\n",
    "mid_singular_values = np.zeros([faces_count, top_values]) \n",
    "for image, label in train_dataloader:\n",
    "    image = image.reshape((W,H)).numpy()\n",
    "    average_matrices[label] += image \n",
    "    counters[label] += 1\n",
    "for i in range(faces_count):\n",
    "  average_matrices[i] = average_matrices[i] / counters[i]  \n",
    "  V, sigma, U = np.linalg.svd(average_matrices[i])\n",
    "  #sigma = sigma/np.linalg.norm(sigma)\n",
    "  mid_singular_values[i] = sigma[:top_values]\n",
    "  mid_singular_values[i] = mid_singular_values[i]/np.linalg.norm(mid_singular_values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mHqNtsn4mW1Y",
    "outputId": "e289ad41-77a2-4693-fa81-91246cd5ae16"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.86576748, 0.44356932, 0.1595048 , 0.11375652, 0.08330053,\n",
       "        0.05497109, 0.0407786 , 0.03942363, 0.03339653, 0.03189616],\n",
       "       [0.82932068, 0.44883773, 0.20713133, 0.14750707, 0.13456547,\n",
       "        0.10554597, 0.07479271, 0.06458367, 0.06315813, 0.05575311],\n",
       "       [0.71167418, 0.63973524, 0.19364909, 0.1430932 , 0.09739949,\n",
       "        0.07906237, 0.06241479, 0.05288104, 0.04889866, 0.038242  ],\n",
       "       [0.76234163, 0.57260645, 0.23820227, 0.10797958, 0.08196609,\n",
       "        0.07775532, 0.05911452, 0.05744569, 0.03955103, 0.03786856],\n",
       "       [0.80830515, 0.51547096, 0.17784604, 0.12893735, 0.10450348,\n",
       "        0.09454569, 0.06660239, 0.06109711, 0.05079446, 0.04549433],\n",
       "       [0.88525576, 0.36033833, 0.18122102, 0.14054707, 0.1059921 ,\n",
       "        0.08639703, 0.08091709, 0.05886712, 0.05572865, 0.0454602 ],\n",
       "       [0.67474048, 0.59190621, 0.33572005, 0.18967529, 0.1446959 ,\n",
       "        0.08563589, 0.07981976, 0.07127688, 0.05822119, 0.05075391]])"
      ]
     },
     "execution_count": 410,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mid_singular_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TR63cSjimW6P",
    "outputId": "5f696b24-800c-4e25-8ff8-a96ebd18a3c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 tensor([5])\n",
      "1 tensor([4])\n",
      "6 tensor([2])\n",
      "5 tensor([5])\n",
      "6 tensor([2])\n",
      "1 tensor([0])\n",
      "6 tensor([6])\n",
      "6 tensor([3])\n",
      "6 tensor([6])\n",
      "6 tensor([3])\n",
      "5 tensor([5])\n",
      "6 tensor([6])\n",
      "6 tensor([3])\n",
      "1 tensor([4])\n",
      "1 tensor([0])\n",
      "1 tensor([1])\n",
      "1 tensor([1])\n",
      "2 tensor([2])\n",
      "1 tensor([4])\n",
      "0.47368421052631576\n"
     ]
    }
   ],
   "source": [
    "count = len(test_dataloader)\n",
    "right = 0\n",
    "for image, label in test_dataloader:\n",
    "  image = image.reshape((W,H)).numpy()\n",
    "  U, sigma, V = np.linalg.svd(image)\n",
    "  sigma = sigma/np.linalg.norm(sigma)\n",
    "  sigma = sigma[:top_values]\n",
    "  distances = []\n",
    "  for i in range(faces_count):\n",
    "    distances.append(distance(mid_singular_values[i],sigma))\n",
    "  distances = np.array(distances)\n",
    "  if (np.argmin(distances) == label):\n",
    "    right += 1\n",
    "  print(np.argmin(distances),label)\n",
    "print(right/count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYiydoxPozBs"
   },
   "source": [
    "#### KNN \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {
    "id": "Mu0z3GBidfTw"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn\n",
    "faces_count = 7\n",
    "W = 112\n",
    "H = 92\n",
    "top_values = 10\n",
    "train_transform = T.Compose([T.Grayscale(),T.ToTensor(), T.Normalize((0.5), (0.5))])\n",
    "test_transform = T.Compose([T.Grayscale(),T.ToTensor(), T.Normalize((0.5), (0.5))])\n",
    "train_dataset = torchvision.datasets.ImageFolder('ORL/train/', transform = train_transform)\n",
    "test_dataset = torchvision.datasets.ImageFolder('ORL/test/', transform = test_transform) \n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=1, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZmUNUHKghIj_",
    "outputId": "afb4e82a-6bd8-480c-c432-e73f82a91d6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['braycurtis',\n",
       " 'canberra',\n",
       " 'chebyshev',\n",
       " 'cityblock',\n",
       " 'correlation',\n",
       " 'cosine',\n",
       " 'cosine',\n",
       " 'dice',\n",
       " 'euclidean',\n",
       " 'hamming',\n",
       " 'haversine',\n",
       " 'jaccard',\n",
       " 'kulsinski',\n",
       " 'l1',\n",
       " 'l2',\n",
       " 'mahalanobis',\n",
       " 'manhattan',\n",
       " 'matching',\n",
       " 'minkowski',\n",
       " 'nan_euclidean',\n",
       " 'precomputed',\n",
       " 'rogerstanimoto',\n",
       " 'russellrao',\n",
       " 'seuclidean',\n",
       " 'sokalmichener',\n",
       " 'sokalsneath',\n",
       " 'sqeuclidean',\n",
       " 'wminkowski',\n",
       " 'yule']"
      ]
     },
     "execution_count": 440,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GpmKU25KmW8s",
    "outputId": "53652315-f4e0-4e51-c5a0-a8809e390031"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='cosine',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 447,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3, metric='cosine')\n",
    "data = []\n",
    "labels = []\n",
    "for image, label in train_dataloader:\n",
    "  image = image.reshape((W,H)).numpy()\n",
    "  U, sigma, V = np.linalg.svd(image)\n",
    "  sigma = sigma/np.linalg.norm(sigma)\n",
    "  data.append(sigma)\n",
    "  labels.append(label)\n",
    "knn.fit(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jyp9A7LVmW4g",
    "outputId": "c881fc2f-275b-4437-a981-cace8c854e5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9523809523809523"
      ]
     },
     "execution_count": 448,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = []\n",
    "test_labels = []\n",
    "for image, label in test_dataloader:\n",
    "  image = image.reshape((W,H)).numpy()\n",
    "  U, sigma, V = np.linalg.svd(image)\n",
    "  sigma = sigma/np.linalg.norm(sigma)\n",
    "  test_data.append(sigma)\n",
    "  test_labels.append(label)\n",
    "y_pred = knn.predict(test_data)\n",
    "sklearn.metrics.accuracy_score(y_pred,test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B925EFt5ovL5"
   },
   "source": [
    "#### FULL ORL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XukZbWlymaDX"
   },
   "outputs": [],
   "source": [
    "!unzip 'drive/MyDrive/SVDproj/ORLFull.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "id": "ZbBsw57qmaIg"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn\n",
    "faces_count = 40\n",
    "W = 112\n",
    "H = 92\n",
    "top_values = 40\n",
    "train_transform = T.Compose([T.Grayscale(),T.ToTensor(), T.Normalize((0.5), (0.5))])\n",
    "test_transform = T.Compose([T.Grayscale(),T.ToTensor(), T.Normalize((0.5), (0.5))])\n",
    "train_dataset = torchvision.datasets.ImageFolder('ORLFull/train/', transform = train_transform)\n",
    "test_dataset = torchvision.datasets.ImageFolder('ORLFull/test/', transform = test_transform) \n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=1, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ksDtdiQgmaLd",
    "outputId": "265da41f-8d24-4de1-ce19-8893b766e9e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 487,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "data = []\n",
    "labels = []\n",
    "for image, label in train_dataloader:\n",
    "  image = image.reshape((W,H)).numpy()\n",
    "  U, sigma, V = np.linalg.svd(image)\n",
    "  data.append(sigma[:top_values])\n",
    "  labels.append(label)\n",
    "knn.fit(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5EJsTdhmaGZ",
    "outputId": "2880ff64-3d4e-4985-f175-b7d682bf8f39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7125"
      ]
     },
     "execution_count": 489,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = []\n",
    "test_labels = []\n",
    "for image, label in test_dataloader:\n",
    "  image = image.reshape((W,H)).numpy()\n",
    "  U, sigma, V = np.linalg.svd(image)\n",
    "  test_data.append(sigma[:top_values])\n",
    "  test_labels.append(label)\n",
    "y_pred = knn.predict(test_data)\n",
    "sklearn.metrics.accuracy_score(y_pred,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZD5Lo26Qm4mh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVefwb6epDJU"
   },
   "source": [
    "#### VGG 4 KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {
    "id": "dLJYSZjopNxG"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn\n",
    "faces_count = 4\n",
    "top_values = 35\n",
    "W = 170\n",
    "R = 512\n",
    "train_transform = T.Compose([T.Grayscale(), T.Resize(R), T.CenterCrop(W), T.ToTensor(), T.Normalize((0.5), (0.5))])\n",
    "test_transform = T.Compose([T.Grayscale(),T.Resize(R), T.CenterCrop(W), T.ToTensor(), T.Normalize((0.5), (0.5))])\n",
    "train_dataset = torchvision.datasets.ImageFolder('vggimageslittle/train/', transform = train_transform)\n",
    "test_dataset = torchvision.datasets.ImageFolder('vggimageslittle/test/', transform = test_transform) \n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,batch_size=1, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3HGRsnMGot2H"
   },
   "outputs": [],
   "source": [
    "\n",
    "data = []\n",
    "labels = []\n",
    "for image, label in tqdm(train_dataloader):\n",
    "  image = image.reshape((image.shape[2],image.shape[3])).numpy()\n",
    "  U, sigma, V = np.linalg.svd(image)\n",
    "  data.append(sigma[:top_values])\n",
    "  labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LbX2v7mtzUgE",
    "outputId": "127d7619-69fb-4b6d-d54a-2261d35cb9e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41.9761    , 33.214397  , 14.796005  , 13.170048  ,  8.850213  ,\n",
       "        7.3001866 ,  6.4989605 ,  5.55059   ,  5.120075  ,  4.710131  ,\n",
       "        3.9689972 ,  3.7742155 ,  3.4223442 ,  3.2599628 ,  2.8512912 ,\n",
       "        2.6140459 ,  2.3682907 ,  2.332227  ,  2.059341  ,  1.9285055 ,\n",
       "        1.8250706 ,  1.7935534 ,  1.6518717 ,  1.6295265 ,  1.4754798 ,\n",
       "        1.4326125 ,  1.3209347 ,  1.269228  ,  1.1772051 ,  1.1237624 ,\n",
       "        1.0632298 ,  1.0318749 ,  1.002482  ,  0.9908491 ,  0.9055075 ,\n",
       "        0.8605176 ,  0.7794064 ,  0.7689727 ,  0.72108656,  0.7088989 ,\n",
       "        0.6754377 ,  0.61172044,  0.6007697 ,  0.5720199 ,  0.51946795,\n",
       "        0.5090528 ,  0.50147235,  0.45825836,  0.4515995 ,  0.4291223 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 516,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cs4LC6agzLVt",
    "outputId": "69362b84-de5f-4c4d-ecc7-64cd3a6a29bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23253, 35)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='euclidean',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 557,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
    "data1 = np.zeros(shape = (len(data), top_values))\n",
    "print(data1.shape)\n",
    "for i in range(len(data)):\n",
    "  data1[i] = data[i][:top_values]\n",
    "knn.fit(data1, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tY1Sz1E4ot7R"
   },
   "outputs": [],
   "source": [
    "test_data = []\n",
    "test_labels = []\n",
    "for image, label in tqdm(test_dataloader):\n",
    "  image = image.reshape((image.shape[2],image.shape[3])).numpy()\n",
    "  U, sigma, V = np.linalg.svd(image)\n",
    "  test_data.append(sigma[:top_values])\n",
    "  test_labels.append(label)\n",
    "y_pred = knn.predict(test_data)\n",
    "sklearn.metrics.accuracy_score(y_pred,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m9rxFbm-ot-k"
   },
   "outputs": [],
   "source": [
    "y_pred = knn.predict(test_data)\n",
    "sklearn.metrics.accuracy_score(y_pred,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "INgmoAo4m4rJ",
    "outputId": "0c544980-ef16-49dd-f6a9-8cf44fe49dcb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 11, 25,  2, 23, 26, 23, 32, 25, 22])"
      ]
     },
     "execution_count": 560,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gI3uv7wi2yAW",
    "outputId": "739d00b4-a601-41c8-de4f-83a536b6f029"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([55]),\n",
       " tensor([48]),\n",
       " tensor([19]),\n",
       " tensor([14]),\n",
       " tensor([7]),\n",
       " tensor([25]),\n",
       " tensor([30]),\n",
       " tensor([14]),\n",
       " tensor([31]),\n",
       " tensor([55])]"
      ]
     },
     "execution_count": 559,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels[:10]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "main2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
